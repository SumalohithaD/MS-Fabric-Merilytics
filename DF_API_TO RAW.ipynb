{"cells":[{"cell_type":"code","execution_count":12,"id":"49d08922-efec-463c-be65-1f6ec54ccd7b","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":"2024-08-01T06:41:47.8362927Z","execution_start_time":"2024-08-01T06:41:20.2860894Z","livy_statement_state":"available","normalized_state":"finished","parent_msg_id":"7556577e-9561-4ebf-8052-759a28cb6fa3","queued_time":"2024-08-01T06:41:19.9004187Z","session_id":"49874dcd-b21e-475f-b061-e527f8982bae","session_start_time":null,"spark_pool":null,"state":"finished","statement_id":14,"statement_ids":[14]},"text/plain":["StatementMeta(, 49874dcd-b21e-475f-b061-e527f8982bae, 14, Finished, Available, Finished)"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:\n","  Could not convert '' with type str: tried to convert to double\n","Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.\n"]}],"source":["spark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\n","spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n","spark.conf.set(\"spark.microsoft.delta.optimizeWrite.binSize\", \"1073741824\")\n","spark.conf.set(\"spark.sql.parquet.int96RebaseModeInRead\", \"LEGACY\")\n","spark.conf.set(\"spark.sql.parquet.int96RebaseModeInWrite\", \"LEGACY\")\n","spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"CORRECTED\")\n","from pyspark.sql.functions import col, when\n","from pyspark.sql import functions as F\n","from pyspark.sql.types import DataType\n","from datetime import datetime\n","import re\n","import os\n","\n","df = spark.sql(\"SELECT * FROM LH_RAW.bronze_crm_data\")\n","\n","# Replace nulls in integer/float columns with 0\n","df_cleaned = df.select([when(col(c).isNull(), 0).otherwise(col(c)).alias(c) \n","                        if t in [\"int\", \"double\", \"float\"] else col(c) for c, t in df.dtypes])\n","\n","# Replace nulls in string columns with 'UNKNOWN'\n","df_cleaned = df_cleaned.select([when(col(c).isNull(), \"UNKNOWN\").otherwise(col(c)).alias(c) \n","                                if t == \"string\" else col(c) for c, t in df_cleaned.dtypes])\n","\n","# Capitalize column names\n","df_cleaned = df_cleaned.select([F.col(c).alias(c.upper()) for c in df_cleaned.columns])\n","\n","# Add \"nav_\" prefix and convert to lowercase\n","final_table_name = f\"silver_crm_data\"\n","\n","# Add a special suffix for the Delta table\n","delta_table_name = f\"{final_table_name}_delta\"\n","\n","# Save the DataFrame as a Delta table with column mapping mode 'name' and schema merge enabled\n","df_cleaned.write.format(\"delta\").mode(\"overwrite\")\\\n","    .option(\"delta.columnMapping.mode\", \"name\")\\\n","    .option(\"mergeSchema\", \"true\")\\\n","    .saveAsTable(delta_table_name)\n"]}],"metadata":{"dependencies":{"lakehouse":{"default_lakehouse":"48bd7c32-ade2-40fe-aa6f-aa1acf6eca87","default_lakehouse_name":"DE_LH_RAW","default_lakehouse_workspace_id":"830b357e-c1db-481a-88a8-91cfdd31edcc","known_lakehouses":[{"id":"48bd7c32-ade2-40fe-aa6f-aa1acf6eca87"}]}},"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"display_name":"Synapse PySpark","language":"Python","name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default"},"synapse_widget":{"state":{},"version":"0.1"},"widgets":{}},"nbformat":4,"nbformat_minor":5}
